{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fcc5041",
   "metadata": {},
   "source": [
    "# Multithreaded Hybrid CHT Model (CHT(updated) + newanalysis)\n",
    "\n",
    "This notebook implements a **multithreaded hybrid** that runs two models in parallel and combines their predictions.\n",
    "\n",
    "\"\n",
    "**Design decisions**\n",
    "- Primary model: `CHT(updated)` (generalizes best across ranges)\n",
    "- Secondary model: `newanalysis` (fallback / cross-check)\n",
    "- Range-driven logic + weighted averaging when models disagree\n",
    "- Classification metrics use ±5% tolerance\n",
    "\n",
    "\"\n",
    "**Usage**\n",
    "\"\n",
    "1. Place the following files in the same folder as this notebook:\n",
    "\"\n",
    "   - `CHT(updated).py` or a notebook `CHT(updated).ipynb` that exports predictions to CSV\n",
    "   - `newanalysis.py` or a notebook `newanalysis.ipynb` that exports predictions to CSV\n",
    "   - Alternatively, provide CSVs: `predictions_CHT_updated.csv`, `predictions_newanalysis.csv` with columns `actual,predicted`.\n",
    "\n",
    "\"\n",
    "If you have model code as importable python modules, ensure they expose a function `predict_all()` (no-arg) that returns `(y_actual, y_pred)` as numpy arrays or lists. The notebook includes robust fallbacks (CSV load) and a multithreaded runner using `concurrent.futures.ThreadPoolExecutor`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ac9e8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Adjust module names or CSV filenames at the top if needed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell: setup imports and config\n",
    "import os, sys, json, math\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, precision_recall_fscore_support\n",
    "%matplotlib inline\n",
    "\n",
    "# File names / module names - adjust if your filenames differ\n",
    "MODULE_PRIMARY = \"CHT.ipynb\"          # expects CHT.ipynb with predict_all()\n",
    "MODULE_SECONDARY = \"newanalysis.ipynb\"        # expects newanalysis.ipynb with predict_all()\n",
    "\n",
    "# CSV fallbacks (if modules not importable)\n",
    "CSV_PRIMARY = \"predictions_CHT.csv\"   # columns: actual,predicted\n",
    "CSV_SECONDARY = \"predictions_newanalysis.csv\"\n",
    "\n",
    "# Hybrid thresholds & params\n",
    "# Temperature ranges as requested (Low, High, Critical, Super-critical, Hyper-critical)\n",
    "RANGES = {\n",
    "    \"Low\": (None, 60),\n",
    "    \"High\": (60, 100),\n",
    "    \"Critical\": (100, 250),\n",
    "    \"Super-critical\": (250, 400),\n",
    "    \"Hyper-critical\": (400, None)\n",
    "}\n",
    "\n",
    "# Tolerance for classification-style metrics\n",
    "TOLERANCE = 0.05  # ±5%\n",
    "# Hybrid weighting when disagreement (give more weight to primary model)\n",
    "PRIMARY_WEIGHT = 0.7\n",
    "SECONDARY_WEIGHT = 0.3\n",
    "\n",
    "print('Setup complete. Adjust module names or CSV filenames at the top if needed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b17ee7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded primary: False secondary: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell: helper functions to load predictions from module or CSV\n",
    "import importlib.util, importlib, inspect\n",
    "\n",
    "def load_from_module(mod_name):\n",
    "    \"\"\"Try to import module mod_name (python file without .py) and call predict_all() or get_predictions().\n",
    "       Returns (actuals, preds) as numpy arrays or None if not available.\"\"\"\n",
    "    try:\n",
    "        # if it's a file path\n",
    "        if os.path.exists(mod_name + \".py\"):\n",
    "            spec = importlib.util.spec_from_file_location(mod_name, mod_name + \".py\")\n",
    "            mod = importlib.util.module_from_spec(spec)\n",
    "            spec.loader.exec_module(mod)\n",
    "        else:\n",
    "            mod = importlib.import_module(mod_name)\n",
    "    except Exception as e:\n",
    "        # import failed\n",
    "        # print(\"Module import failed:\", e)\n",
    "        return None\n",
    "    # Look for predict_all or get_predictions\n",
    "    for fn in (\"predict_all\", \"get_predictions\", \"predict\"):\n",
    "        if hasattr(mod, fn) and callable(getattr(mod, fn)):\n",
    "            try:\n",
    "                out = getattr(mod, fn)()\n",
    "                # allow (y_true, y_pred) or dict / dataframe\n",
    "                if isinstance(out, tuple) and len(out) >= 2:\n",
    "                    y_true, y_pred = np.array(out[0]), np.array(out[1])\n",
    "                    return y_true, y_pred\n",
    "                if isinstance(out, dict):\n",
    "                    y_true = np.array(out.get('actual') or out.get('y_true') or out.get('y_test'))\n",
    "                    y_pred = np.array(out.get('predicted') or out.get('y_pred') or out.get('y_hat'))\n",
    "                    return y_true, y_pred\n",
    "                # dataframe\n",
    "                if hasattr(out, 'shape') and out.shape[1] >= 2:\n",
    "                    arr = np.array(out)\n",
    "                    return arr[:,0], arr[:,1]\n",
    "            except Exception as e:\n",
    "                # print('predict fn failed:', e)\n",
    "                return None\n",
    "    # fallback: look for variables y_test, y_pred in module\n",
    "    for var in (\"y_test\",\"y_true\",\"actuals\",\"y_actual\"):\n",
    "        for varp in (\"y_pred\",\"y_hat\",\"predicted\"):\n",
    "            if hasattr(mod, var) and hasattr(mod, varp):\n",
    "                try:\n",
    "                    return np.array(getattr(mod,var)), np.array(getattr(mod,varp))\n",
    "                except Exception:\n",
    "                    pass\n",
    "    return None\n",
    "\n",
    "def load_from_csv(csv_path):\n",
    "    if not os.path.exists(csv_path):\n",
    "        return None\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Accept columns names in variety\n",
    "    col_actual = None\n",
    "    for c in ('actual','y_true','y_test','true'):\n",
    "        if c in df.columns:\n",
    "            col_actual = c; break\n",
    "    col_pred = None\n",
    "    for c in ('predicted','y_pred','y_hat','pred'):\n",
    "        if c in df.columns:\n",
    "            col_pred = c; break\n",
    "    if col_actual is None or col_pred is None:\n",
    "        return None\n",
    "    return df[col_actual].values, df[col_pred].values\n",
    "\n",
    "def load_predictions(primary_module, secondary_module, csv_primary, csv_secondary):\n",
    "    # Try modules first\n",
    "    primary = load_from_module(primary_module)\n",
    "    secondary = load_from_module(secondary_module)\n",
    "    if primary is None:\n",
    "        primary = load_from_csv(csv_primary)\n",
    "    if secondary is None:\n",
    "        secondary = load_from_csv(csv_secondary)\n",
    "    return primary, secondary\n",
    "\n",
    "# quick test load (won't error if files missing)\n",
    "p, s = load_predictions(MODULE_PRIMARY, MODULE_SECONDARY, CSV_PRIMARY, CSV_SECONDARY)\n",
    "print('Loaded primary:', bool(p), 'secondary:', bool(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de852e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary available: False Secondary available: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell: multithreaded prediction runner\n",
    "def run_models_multithread(primary_loader, secondary_loader):\n",
    "    \"\"\"primary_loader and secondary_loader are callables that return (y_true, y_pred).\n",
    "       They are run in parallel threads and results returned.\"\"\"\n",
    "    results = {}\n",
    "    with ThreadPoolExecutor(max_workers=2) as ex:\n",
    "        futures = {}\n",
    "        futures[ex.submit(primary_loader)] = 'primary'\n",
    "        futures[ex.submit(secondary_loader)] = 'secondary'\n",
    "        for fut in as_completed(futures):\n",
    "            key = futures[fut]\n",
    "            try:\n",
    "                out = fut.result()\n",
    "                results[key] = out  # may be None\n",
    "            except Exception as e:\n",
    "                results[key] = None\n",
    "    return results\n",
    "\n",
    "# Define loader wrappers\n",
    "def primary_loader():\n",
    "    return load_from_module(MODULE_PRIMARY) or load_from_csv(CSV_PRIMARY)\n",
    "\n",
    "def secondary_loader():\n",
    "    return load_from_module(MODULE_SECONDARY) or load_from_csv(CSV_SECONDARY)\n",
    "\n",
    "res = run_models_multithread(primary_loader, secondary_loader)\n",
    "print('Primary available:', bool(res.get('primary')), 'Secondary available:', bool(res.get('secondary')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "247a2e5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No predictions available from primary or secondary loaders. Provide CSVs or importable modules.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m res_secondary = res.get(\u001b[33m'\u001b[39m\u001b[33msecondary\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m res_primary \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m res_secondary \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mNo predictions available from primary or secondary loaders. Provide CSVs or importable modules.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Choose the source of ground-truth actuals: prefer primary then secondary\u001b[39;00m\n\u001b[32m     11\u001b[39m actuals = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: No predictions available from primary or secondary loaders. Provide CSVs or importable modules."
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell: hybrid combination logic (range-based routing + weighted averaging on disagreement)\n",
    "# After multithreaded run, combine results into hybrid prediction array\n",
    "\n",
    "res_primary = res.get('primary')\n",
    "res_secondary = res.get('secondary')\n",
    "\n",
    "if res_primary is None and res_secondary is None:\n",
    "    raise RuntimeError('No predictions available from primary or secondary loaders. Provide CSVs or importable modules.')\n",
    "\n",
    "# Choose the source of ground-truth actuals: prefer primary then secondary\n",
    "actuals = None\n",
    "if res_primary is not None and len(res_primary)>=2:\n",
    "    actuals = np.array(res_primary[0])\n",
    "elif res_secondary is not None and len(res_secondary)>=2:\n",
    "    actuals = np.array(res_secondary[0])\n",
    "else:\n",
    "    raise RuntimeError('Could not determine actuals from model outputs.')\n",
    "\n",
    "# align lengths\n",
    "if res_primary is not None:\n",
    "    y_pred_primary = np.array(res_primary[1])[:len(actuals)]\n",
    "else:\n",
    "    y_pred_primary = None\n",
    "if res_secondary is not None:\n",
    "    y_pred_secondary = np.array(res_secondary[1])[:len(actuals)]\n",
    "else:\n",
    "    y_pred_secondary = None\n",
    "\n",
    "n = len(actuals)\n",
    "hybrid_pred = np.full(n, np.nan)\n",
    "\n",
    "def get_range_label(t):\n",
    "    for r,(lo,hi) in RANGES.items():\n",
    "        if lo is None and t < hi: return r\n",
    "        if hi is None and t >= lo: return r\n",
    "        if lo is not None and hi is not None and lo <= t < hi: return r\n",
    "    return 'Unknown'\n",
    "\n",
    "# combine per-sample\n",
    "for i, a in enumerate(actuals):\n",
    "    r = get_range_label(a)\n",
    "    # simple rule: prefer primary unless range is critical/hyper-critical and secondary exists and differs significantly\n",
    "    if y_pred_primary is None:\n",
    "        hybrid_pred[i] = y_pred_secondary[i]\n",
    "        continue\n",
    "    if y_pred_secondary is None:\n",
    "        hybrid_pred[i] = y_pred_primary[i]\n",
    "        continue\n",
    "    p = y_pred_primary[i]\n",
    "    s = y_pred_secondary[i]\n",
    "    # if both agree within 3% of actual -> take weighted avg (favor primary)\n",
    "    if abs(p - s) <= 0.03 * max(abs(a),1):\n",
    "        hybrid_pred[i] = PRIMARY_WEIGHT * p + SECONDARY_WEIGHT * s\n",
    "        continue\n",
    "    # If in higher-risk ranges (Critical, Super-critical, Hyper-critical), trust secondary more if it predicts higher safety-critical response\n",
    "    if r in ('Critical','Super-critical','Hyper-critical'):\n",
    "        # choose the prediction closer to actual if known historic accuracy; else favor secondary by small margin\n",
    "        # Here we use weighted average favoring secondary slightly in risky ranges\n",
    "        hybrid_pred[i] = 0.4 * p + 0.6 * s\n",
    "    else:\n",
    "        # Low/High ranges: favor primary\n",
    "        hybrid_pred[i] = PRIMARY_WEIGHT * p + SECONDARY_WEIGHT * s\n",
    "\n",
    "print('Hybrid prediction array constructed, length =', len(hybrid_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa9634d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell: compute metrics per model and for hybrid, plus per-range metrics and breaking points\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, precision_recall_fscore_support\n",
    "\n",
    "def regression_metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return {'MAE': mae, 'RMSE': rmse, 'R2': r2}\n",
    "\n",
    "def within_tol(y_true, y_pred, tol=TOLERANCE):\n",
    "    return np.abs(y_true - y_pred) <= (tol * np.maximum(np.abs(y_true),1e-6))\n",
    "\n",
    "models_results = {}\n",
    "# primary\n",
    "if y_pred_primary is not None:\n",
    "    models_results['primary'] = {'pred': y_pred_primary, 'reg': regression_metrics(actuals, y_pred_primary),\n",
    "                                 'acc5': float(np.mean(within_tol(actuals, y_pred_primary)))}\n",
    "# secondary\n",
    "if y_pred_secondary is not None:\n",
    "    models_results['secondary'] = {'pred': y_pred_secondary, 'reg': regression_metrics(actuals, y_pred_secondary),\n",
    "                                   'acc5': float(np.mean(within_tol(actuals, y_pred_secondary)))}\n",
    "# hybrid\n",
    "models_results['hybrid'] = {'pred': hybrid_pred, 'reg': regression_metrics(actuals, hybrid_pred),\n",
    "                            'acc5': float(np.mean(within_tol(actuals, hybrid_pred)))}\n",
    "\n",
    "# per-range metrics and breaking points\n",
    "per_range = {k: {} for k in RANGES.keys()}\n",
    "breaking_points = {k: {} for k in ['primary','secondary','hybrid']}\n",
    "\n",
    "for name, info in models_results.items():\n",
    "    y_pred = info['pred']\n",
    "    per_range[name] = {}\n",
    "    for r,(lo,hi) in RANGES.items():\n",
    "        idx = [i for i,t in enumerate(actuals) if (lo is None or t>=lo) and (hi is None or t<hi)]\n",
    "        if not idx:\n",
    "            per_range[name][r] = None\n",
    "            continue\n",
    "        yt = actuals[idx]\n",
    "        yp = y_pred[idx]\n",
    "        reg = regression_metrics(yt, yp)\n",
    "        acc5 = float(np.mean(within_tol(yt, yp)))\n",
    "        per_range[name][r] = {'regression': reg, 'acc5': acc5}\n",
    "        # breaking points: error > mean + 2*std\n",
    "        errs = np.abs(yp - yt)\n",
    "        thr = errs.mean() + 2*errs.std()\n",
    "        breaks = [{'index': int(idx[i]), 'actual': float(yt[i]), 'predicted': float(yp[i]), 'error': float(errs[i])}\n",
    "                  for i in range(len(errs)) if errs[i] > thr]\n",
    "        breaking_points[name][r] = breaks\n",
    "\n",
    "# summary print\n",
    "import pandas as pd\n",
    "rows = []\n",
    "for name, info in models_results.items():\n",
    "    reg = info['reg']\n",
    "    rows.append({'Model': name, 'MAE': reg['MAE'], 'RMSE': reg['RMSE'], 'R2': reg['R2'], 'Accuracy±5%': info['acc5']})\n",
    "df_summary = pd.DataFrame(rows).sort_values('MAE')\n",
    "print(df_summary.to_string(index=False))\n",
    "\n",
    "# save summary and breaking points\n",
    "os.makedirs('hybrid_report', exist_ok=True)\n",
    "df_summary.to_csv('hybrid_report/summary_metrics.csv', index=False)\n",
    "with open('hybrid_report/breaking_points.json','w',encoding='utf-8') as f:\n",
    "    json.dump(breaking_points, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75addb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell: Visualizations (actual vs predictions + error histograms + per-range MAE)\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(actuals, label='Actual', linewidth=2)\n",
    "if y_pred_primary is not None:\n",
    "    plt.plot(y_pred_primary, label='Primary (CHT_updated)', alpha=0.8)\n",
    "if y_pred_secondary is not None:\n",
    "    plt.plot(y_pred_secondary, label='Secondary (newanalysis)', alpha=0.8)\n",
    "plt.plot(hybrid_pred, label='Hybrid', linestyle='--', linewidth=2)\n",
    "plt.legend()\n",
    "plt.title('Actual vs Predictions (Primary, Secondary, Hybrid)')\n",
    "plt.xlabel('Sample index')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('hybrid_report/actual_vs_pred.png', dpi=200)\n",
    "plt.show()\n",
    "\n",
    "# Error histograms\n",
    "for name,info in [('primary', y_pred_primary), ('secondary', y_pred_secondary), ('hybrid', hybrid_pred)]:\n",
    "    if info is None:\n",
    "        continue\n",
    "    errs = np.abs(info - actuals)\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.hist(errs, bins=40)\n",
    "    plt.title(f'Absolute Error Distribution - {name}')\n",
    "    plt.xlabel('Absolute error (°C)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'hybrid_report/error_dist_{name}.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "# Per-range MAE chart\n",
    "for r,(lo,hi) in RANGES.items():\n",
    "    names = []\n",
    "    maes = []\n",
    "    for name,info in models_results.items():\n",
    "        idx = [i for i,t in enumerate(actuals) if (lo is None or t>=lo) and (hi is None or t<hi)]\n",
    "        if not idx: continue\n",
    "        yt = actuals[idx]; yp = info['pred'][idx]\n",
    "        names.append(name); maes.append(mean_absolute_error(yt, yp))\n",
    "    if names:\n",
    "        plt.figure(figsize=(8,4))\n",
    "        plt.bar(names, maes)\n",
    "        plt.title(f'MAE in range {r}')\n",
    "        plt.ylabel('MAE (°C)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'hybrid_report/mae_{r}.png', dpi=150)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbb7bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell: display report assets and final remarks\n",
    "from IPython.display import Image, display\n",
    "print('Summary table saved to hybrid_report/summary_metrics.csv')\n",
    "display(Image('hybrid_report/actual_vs_pred.png'))\n",
    "for img in sorted([p for p in os.listdir('hybrid_report') if p.startswith('error_dist') or p.startswith('mae_')]):\n",
    "    display(Image(os.path.join('hybrid_report', img)))\n",
    "\n",
    "print('\\nBreaking points saved to hybrid_report/breaking_points.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45a8c29",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusions & Recommendations\n",
    "\n",
    "\"\n",
    "- This multithreaded hybrid runs both models in parallel and combines predictions using range-aware logic and weighted averaging.\n",
    "\"\n",
    "- Use `CHT(updated)` as the primary predictor for general operation.\n",
    "\"\n",
    "- Use `newanalysis` as a secondary/fallback in the high/critical ranges.\n",
    "\"\n",
    "- The hybrid favors the primary model, but leans more on the secondary in critical/super-critical ranges where safety is important.\n",
    "\n",
    "\"\n",
    "**Next steps for research paper:**\n",
    "\"\n",
    "1. Export the generated plots and summary CSV into your manuscript figures/tables.\n",
    "\"\n",
    "2. Optionally tune weights (PRIMARY_WEIGHT) or create a data-driven cutoff.\n",
    "\"\n",
    "3. Add confidence calibration — e.g., report per-sample prediction uncertainty and mark uncertain predictions in the dataset.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
